{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating NER Models\n",
    "## Purpose:\n",
    "* Create sample Name Entity Recognition model from scratch (i.e. not NLTK implementation)\n",
    "* Phase 2: Machine Learning\n",
    "\n",
    "### Relevant Links:\n",
    "- Data Repo\n",
    "    * https://github.com/davidsbatista/NER-datasets/tree/master/CONLL2003\n",
    "- Description of datasets used historically for NER\n",
    "    * https://www.clips.uantwerpen.be/conll2003/ner/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all data\n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "val_df = pd.read_csv('valid_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with simple Naive Bayes multi-class classification\n",
    "# let's combine the pre-defined train/test/val sets so that we\n",
    "# can have more train/test data (i.e. no val data)\n",
    "\n",
    "# val data will be more useful if we want to train model\n",
    "# with a neural network approach\n",
    "df = pd.concat([train_df, test_df, val_df],axis=0)\\\n",
    "       .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X_y(df):\n",
    "    \"\"\"Docstring: quickly parse through data to split vectorized text as feature vector X\n",
    "    and label data (NER labels) y for machine learning models\"\"\"\n",
    "    # vectorize words - this is essentially a one hot encoding for each word\n",
    "    # into it's own n-dimensional space\n",
    "    v = DictVectorizer(sparse=False)\n",
    "    # fit/transform our data\n",
    "    X = v.fit_transform(df.to_dict('records'))\n",
    "    # return label values\n",
    "    y = df.NER.values\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature vectors with encoded vectors and corresponding labels\n",
    "X, y = split_X_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with a simple Naive Bayes classifier\n",
    "nb = MultinomialNB(alpha=0.01) # instantiate model\n",
    "# train with a partial fit to not load all data into memory. can rerun this later with more data\n",
    "nb.partial_fit(X_train, y_train, classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class names used for classification report\n",
    "classes = df.NER.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-ORG       1.00      1.00      1.00      3053\n",
      "      B-MISC       1.00      1.00      1.00      1711\n",
      "       B-PER       1.00      1.00      1.00      3360\n",
      "       I-PER       1.00      1.00      1.00      2240\n",
      "       B-LOC       1.00      1.00      1.00      3498\n",
      "       I-ORG       1.00      1.00      1.00      1602\n",
      "      I-MISC       1.00      1.00      1.00       524\n",
      "       I-LOC       1.00      1.00      1.00       549\n",
      "\n",
      "    accuracy                           1.00     16537\n",
      "   macro avg       1.00      1.00      1.00     16537\n",
      "weighted avg       1.00      1.00      1.00     16537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred=nb.predict(X_test), y_true=y_test, labels = classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model has high precision, recall and accuracy\n",
    "* Multinomial Naive Bayes worked fairly well with this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionalize model so that we can run in Flask API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nb_ner.sav']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"nb_ner.sav\"\n",
    "joblib.dump(nb, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize words into feature vector set\n",
    "vectorized = split_X_y(df)[0]\n",
    "# convert to a pandas DataFrame and add the original word to use as a look-up table\n",
    "word_vectors = pd.DataFrame(vectorized)\n",
    "word_vectors['Word'] = df['Word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for quick uploading into Flask API\n",
    "word_vectors.to_csv('word_vectors.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_vectors.joblib']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "f = 'word_vectors.joblib'\n",
    "joblib.dump(word_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load naive bayes model from disk\n",
    "model = joblib.load(filename)\n",
    "\n",
    "def make_NER_prediction(string, word_vectors):\n",
    "    \"\"\"Docstring: make a prediction on a target word. If the word is in our corpus, \n",
    "    the model provides the NER. Otherwise, the model provides'O'.\n",
    "    \n",
    "    This function uses the Naive Bayes model trained previously. \n",
    "    \n",
    "    Inputs: string - input string to find NER\n",
    "            word_vectors - word vectors set (saved as word_vectors.csv)\n",
    "            \n",
    "    Outputs: type string object with either NER prediction or 'O' for out of scope\"\"\"\n",
    "    \n",
    "    if string in word_vectors.Word.tolist():\n",
    "        # if the word is in our corpus, grab the vector. there could be multiple occurances\n",
    "        # so we are grabbing the mean of this vector\n",
    "        x = word_vectors.loc[word_vectors.Word==string].mean().values\n",
    "        # this vector is of the right dimensions that the model was fit on\n",
    "        # now we can make a prediction\n",
    "        pred = model.predict(x.reshape(1,-1))[0]\n",
    "        return pred\n",
    "    else:\n",
    "        return \"O\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample implementation\n",
    "\n",
    "sentence = \"Jack lives in London\".split(' ')\n",
    "NER = []\n",
    "for word in sentence:\n",
    "    # make prediction\n",
    "    NER.append(make_NER_prediction(word, word_vectors))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-PER', 'O', 'O', 'B-LOC']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
